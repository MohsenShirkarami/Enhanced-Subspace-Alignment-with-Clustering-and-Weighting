{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn441NPF4yk6"
      },
      "source": [
        "# **Core: Subspace Alignment (SA)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ6c_YcY4sbe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def subspace_alignment(source_data, target_data, subspace_dim):\n",
        "  # Convert input data to numpy arrays\n",
        "  source_data = np.array(source_data)\n",
        "  target_data = np.array(target_data)\n",
        "\n",
        "  # Define the normalize_data function\n",
        "  def normalize_data(data):\n",
        "    mean_data = np.mean(data, axis=0)\n",
        "    std_data = np.std(data, axis=0)\n",
        "    normalized_data = (data - mean_data) / std_data\n",
        "    return normalized_data\n",
        "\n",
        "  # Normalize source and target data\n",
        "  source_data_norm = normalize_data(source_data)\n",
        "  target_data_norm = normalize_data(target_data)\n",
        "\n",
        "  # Perform PCA on source and target data\n",
        "  def perform_pca(data):\n",
        "    cov_matrix = np.cov(data.T)\n",
        "    eig_val, eig_vec = np.linalg.eig(cov_matrix)\n",
        "    idx = eig_val.argsort()[::-1]\n",
        "    eig_val = eig_val[idx]\n",
        "    eig_vec = eig_vec[:, idx]\n",
        "    return eig_vec\n",
        "\n",
        "  # Compute PCA for source and target data\n",
        "  xs = perform_pca(source_data_norm)\n",
        "  xt = perform_pca(target_data_norm)\n",
        "\n",
        "  # Generating the subspaces\n",
        "  subspace_dim = min(subspace_dim, min(xs.shape[1], xt.shape[1]))\n",
        "  xs = xs[:, :subspace_dim]\n",
        "  xt = xt[:, :subspace_dim]\n",
        "\n",
        "  # Subspace alignment and project data\n",
        "  target_aligned_source_data = source_data_norm @ (xs @ xs.T @ xt)\n",
        "  target_projected_data = target_data_norm @ xt\n",
        "  target_subspace = xt\n",
        "\n",
        "  return target_aligned_source_data, target_projected_data, target_subspace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gknD6Z64-n8"
      },
      "source": [
        "# **Main Code: Enhanced Subspace Alignment with Clustering and Weighting**\n",
        "* Working on SEED dataset\n",
        "* Target Subjects: 1-5\n",
        "* **Important Note**: Parallel versions should be used for other subjects!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5maUCjNi49QX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import random as python_random\n",
        "import random as rn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, SpectralClustering\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.utils import class_weight, shuffle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import scipy.io\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.math import confusion_matrix\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "\n",
        "# Set the random seeds\n",
        "os.environ['PYTHONHASHSEED']= '123'\n",
        "os.environ['TF_CUDNN_DETERMINISTIC']= '1'\n",
        "np.random.seed(123)\n",
        "python_random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "SEED_num = 40\n",
        "\n",
        "sel_ch = np.arange(1,63,1)\n",
        "num_ch = 62\n",
        "for i in range(0,len(sel_ch)):\n",
        "  sel_ch[i] = sel_ch[i] - 1\n",
        "\n",
        "labels = scipy.io.loadmat('/content/drive/MyDrive/SEED/ExtractedFeatures/label.mat')\n",
        "\n",
        "# Paths for preprocessed data\n",
        "path_sess_1  = ['1_20131027.mat','2_20140404.mat','3_20140603.mat','4_20140621.mat','5_20140411.mat',\n",
        "                '6_20130712.mat','7_20131027.mat','8_20140511.mat','9_20140620.mat','10_20131130.mat',\n",
        "                '11_20140618.mat','12_20131127.mat','13_20140527.mat','14_20140601.mat','15_20130709.mat']\n",
        "\n",
        "path_sess_2  = ['1_20131030.mat','2_20140413.mat','3_20140611.mat','4_20140702.mat','5_20140418.mat',\n",
        "                '6_20131016.mat','7_20131030.mat','8_20140514.mat','9_20140627.mat','10_20131204.mat',\n",
        "                '11_20140625.mat','12_20131201.mat','13_20140603.mat','14_20140615.mat','15_20131016.mat']\n",
        "\n",
        "path_sess_3  = ['1_20131107.mat','2_20140419.mat','3_20140629.mat','4_20140705.mat','5_20140506.mat',\n",
        "                '6_20131113.mat','7_20131106.mat','8_20140521.mat','9_20140704.mat','10_20131211.mat',\n",
        "                '11_20140630.mat','12_20131207.mat','13_20140610.mat','14_20140627.mat','15_20131105.mat']\n",
        "\n",
        "# num_feature is a variable to select different features ----> DE feature: num_feature=4\n",
        "num_feature = 4\n",
        "label_sess_1 = labels['label']\n",
        "\n",
        "# Select values ​​for parameters to execute the code\n",
        "sub_dim = 40\n",
        "Source_SESS_A = 1\n",
        "Source_SESS_B = 3\n",
        "Target_SESS = 2\n",
        "\n",
        "print(\"Source_Session_A = \"+str(Source_SESS_A) +\"  +  Source_Session_B = \"+str(Source_SESS_B)+\"  >>>>>>>>>>>  Traget_Session = \"+str(Target_SESS)+\"\\n\\n\")\n",
        "\n",
        "PATH = [path_sess_1, path_sess_2, path_sess_3]\n",
        "path_Source_SESS_A = PATH[Source_SESS_A-1]\n",
        "path_Source_SESS_B = PATH[Source_SESS_B-1]\n",
        "path_Target_SESS = PATH[Target_SESS-1]\n",
        "\n",
        "#Loading and Preparing the Cross-Subject Accuracy Matrix According to the Selected Targer Session\n",
        "cross_dir = '/content/drive/MyDrive/SubjectClustering_SEED_AllSessions/Cross_Acc_OneSessionTrainTrain'+str(Target_SESS)+'.csv'\n",
        "ACC_MAT= np.genfromtxt(cross_dir, delimiter=',')\n",
        "ACC_MAT = ACC_MAT.reshape(15,15,5)\n",
        "\n",
        "ACC_MAT_iter = np.zeros((15,5))\n",
        "Target_acc_list_selected_MMPP = []\n",
        "Target_acc_list_selected_MNCC = []\n",
        "Target_acc_list_selected_MMA = []\n",
        "Target_acc_list_selected_MTA = []\n",
        "Target_acc_list_clustering = []\n",
        "Target_acc_list_HIGHEST = []\n",
        "\n",
        "# Important Note: This version of the code is prepared for the first 5 subjects.\n",
        "# Parallel versions should be used for other subjects!\n",
        "\n",
        "for target_sub in range(1,6):\n",
        "\n",
        "  ############ 1. Source Subjects Clustering ############\n",
        "  source_sub = np.concatenate((np.arange(1,target_sub),np.arange(target_sub+1, 16)),0)\n",
        "  print(\"\\n\\nTarget Subject: \"+str(target_sub))\n",
        "  print(\"Source Subjects: \"+str(source_sub)+\"\\n\")\n",
        "  iter_acc_list_selected_MMPP = []\n",
        "  iter_acc_list_selected_MNCC = []\n",
        "  iter_acc_list_selected_MMA = []\n",
        "  iter_acc_list_selected_MTA = []\n",
        "  iter_acc_list_clustering = []\n",
        "  iter_acc_list_HIGHEST = []\n",
        "\n",
        "  for iter in range(0,5):\n",
        "    all_trials = [*range(0,15)]\n",
        "    valid_trials = [*range(3*iter,3*(iter+1))]\n",
        "    train_trials = np.concatenate((np.arange(0,3*iter),np.arange((3*iter+3), 15)),0)\n",
        "    print(\"\\niteration \"+str(iter+1)+\" started!\")\n",
        "    print(\"\\ntrian trials: \" + str(train_trials))\n",
        "    print(\"valid trials: \" + str(valid_trials)+\"\\n\")\n",
        "\n",
        "    ACC_MAT_iteration = ACC_MAT[:,:,iter]\n",
        "    Cross_Acc = np.delete(ACC_MAT_iteration,target_sub-1,0)\n",
        "    Cross_Acc = np.delete(Cross_Acc,target_sub-1,1)\n",
        "\n",
        "    SILHO =[]\n",
        "    for num_cluster in range(2,5):\n",
        "      Kmean = KMeans(n_clusters=num_cluster, random_state=0)\n",
        "      cluster_labels = Kmean.fit_predict(Cross_Acc)\n",
        "      silhouette_avg = silhouette_score(Cross_Acc, cluster_labels)\n",
        "      SILHO.append(silhouette_avg)\n",
        "    SILHO = np.array(SILHO)\n",
        "    print(\"silhouette array:\" + str(SILHO))\n",
        "    best_num_cluster = np.argmax(SILHO) + 2\n",
        "    print(\"the best number of clusters: \" +str(best_num_cluster))\n",
        "\n",
        "    Kmean = KMeans(n_clusters=best_num_cluster, random_state=0)\n",
        "    cluster_labels = Kmean.fit_predict(Cross_Acc)\n",
        "    print(\"cluster labels: \"+str(cluster_labels))\n",
        "    clusters_source_index =[]\n",
        "    num_cluster_samples =[]\n",
        "    for cluster in range(0,best_num_cluster):\n",
        "      cluster_list = np.where(np.equal(cluster_labels, cluster))\n",
        "      cluster_list = np.array(cluster_list)\n",
        "      cluster_list = np.squeeze(cluster_list)\n",
        "      if cluster_list.shape ==():\n",
        "        temp = cluster_list\n",
        "        x = temp.astype(int)\n",
        "        cluster_list = np.array([x])\n",
        "      list_sub = list(cluster_list)\n",
        "      clusters_source_index.append(list_sub)\n",
        "      num_cluster_samples.append(len(list_sub))\n",
        "    clusters_index = np.array(clusters_source_index)\n",
        "    cluster_num_of_sources = np.array(num_cluster_samples)\n",
        "    print(\"number of subjects in clusters: \" +str(num_cluster_samples))\n",
        "    print(\"cluster_source_index :\" +str(clusters_source_index))\n",
        "\n",
        "    # Cluster Selection based on the Clustering Method\n",
        "    columns = np.concatenate((np.arange(0,target_sub-1),np.arange((target_sub), 15)),0)\n",
        "    data_target = ACC_MAT_iteration[target_sub-1,columns]\n",
        "    data_target = data_target.reshape(1,-1)\n",
        "    K_clustering = int(Kmean.predict(data_target))\n",
        "    print(\"K (Selected Cluster): \"+str(K_clustering+1))\n",
        "    num_of_SubSess = num_cluster_samples[K_clustering]\n",
        "    index_of_SubSess = clusters_source_index[K_clustering]\n",
        "\n",
        "    Source_Subspace = np.zeros((14,num_ch,sub_dim))\n",
        "    subj_row = 0\n",
        "    init_A = 1   # for features matrix initialization\n",
        "    init_B = 1   # for features matrix initialization\n",
        "    init_Target = 1  # for features matrix initialization\n",
        "\n",
        "    # Preparation the samples\n",
        "    for p in source_sub:\n",
        "      ######################### Sourcce Session A ##################################\n",
        "      fetures_Source_SESS_A  = scipy.io.loadmat('/content/drive/MyDrive/SEED/ExtractedFeatures/' + path_Source_SESS_A[p-1])\n",
        "      feature_delta = []\n",
        "      feature_tetha = []\n",
        "      feature_alpha = []\n",
        "      feature_beta  = []\n",
        "      feature_gamma = []\n",
        "\n",
        "      field_name = []\n",
        "      for key in fetures_Source_SESS_A.keys():\n",
        "          field_name.append(key)\n",
        "\n",
        "      label = []\n",
        "      for i in all_trials:\n",
        "        data_trial = fetures_Source_SESS_A[field_name[12*i+num_feature]]\n",
        "        for k in range(140,data_trial.shape[1]):\n",
        "          data_epoch   = data_trial[:,k,:]\n",
        "          data_epoch   = np.squeeze(data_epoch)\n",
        "          a = []\n",
        "          b = []\n",
        "          c = []\n",
        "          d = []\n",
        "          e = []\n",
        "          for j in range(0,num_ch):\n",
        "            data_channel = data_epoch[sel_ch[j],:]\n",
        "            a.append(data_channel[0])\n",
        "            b.append(data_channel[1])\n",
        "            c.append(data_channel[2])\n",
        "            d.append(data_channel[3])\n",
        "            e.append(data_channel[4])\n",
        "\n",
        "          feature_delta.append(np.array(a))\n",
        "          feature_tetha.append(np.array(b))\n",
        "          feature_alpha.append(np.array(c))\n",
        "          feature_beta.append(np.array(d))\n",
        "          feature_gamma.append(np.array(e))\n",
        "          label.append(label_sess_1[0,i])\n",
        "\n",
        "      feature_delta = np.array(feature_delta)\n",
        "      feature_tetha = np.array(feature_tetha)\n",
        "      feature_alpha = np.array(feature_alpha)\n",
        "      feature_beta  = np.array(feature_beta)\n",
        "      feature_gamma = np.array(feature_gamma)\n",
        "      label        = np.array(label)\n",
        "      all_features = feature_gamma\n",
        "\n",
        "      num_sample = all_features.shape[0]\n",
        "      if init_A==1:\n",
        "        raw_sources_features_A = np.zeros((14,num_sample,num_ch))\n",
        "        raw_sources_label_A = np.zeros((14,num_sample))\n",
        "        init_A = 0\n",
        "\n",
        "      ## label and data for Source Session A set\n",
        "      data_train_S_A  = all_features\n",
        "      label_train_S_A = label\n",
        "\n",
        "      raw_sources_features_A[subj_row,:,:] = data_train_S_A\n",
        "      raw_sources_label_A[subj_row,:] = label_train_S_A\n",
        "\n",
        "      ######################### Source Session B ##################################\n",
        "      fetures_Source_SESS_B  = scipy.io.loadmat('/content/drive/MyDrive/SEED/ExtractedFeatures/' + path_Source_SESS_B[p-1])\n",
        "      feature_delta = []\n",
        "      feature_tetha = []\n",
        "      feature_alpha = []\n",
        "      feature_beta  = []\n",
        "      feature_gamma = []\n",
        "\n",
        "      field_name = []\n",
        "      for key in fetures_Source_SESS_B.keys():\n",
        "          field_name.append(key)\n",
        "\n",
        "      label = []\n",
        "      for i in all_trials:\n",
        "        data_trial = fetures_Source_SESS_B[field_name[12*i+num_feature]]\n",
        "        for k in range(140,data_trial.shape[1]):\n",
        "          data_epoch   = data_trial[:,k,:]\n",
        "          data_epoch   = np.squeeze(data_epoch)\n",
        "          a = []\n",
        "          b = []\n",
        "          c = []\n",
        "          d = []\n",
        "          e = []\n",
        "          for j in range(0,num_ch):\n",
        "            data_channel = data_epoch[sel_ch[j],:]\n",
        "            a.append(data_channel[0])\n",
        "            b.append(data_channel[1])\n",
        "            c.append(data_channel[2])\n",
        "            d.append(data_channel[3])\n",
        "            e.append(data_channel[4])\n",
        "\n",
        "          feature_delta.append(np.array(a))\n",
        "          feature_tetha.append(np.array(b))\n",
        "          feature_alpha.append(np.array(c))\n",
        "          feature_beta.append(np.array(d))\n",
        "          feature_gamma.append(np.array(e))\n",
        "          label.append(label_sess_1[0,i])\n",
        "\n",
        "      feature_delta = np.array(feature_delta)\n",
        "      feature_tetha = np.array(feature_tetha)\n",
        "      feature_alpha = np.array(feature_alpha)\n",
        "      feature_beta  = np.array(feature_beta)\n",
        "      feature_gamma = np.array(feature_gamma)\n",
        "      label        = np.array(label)\n",
        "      all_features = feature_gamma\n",
        "\n",
        "      num_sample = all_features.shape[0]\n",
        "      if init_B==1:\n",
        "        raw_sources_features_B = np.zeros((14,num_sample,num_ch))\n",
        "        raw_sources_label_B = np.zeros((14,num_sample))\n",
        "        init_B = 0\n",
        "\n",
        "      ## label and data for Source Session B set\n",
        "      data_train_S_B  = all_features\n",
        "      label_train_S_B = label\n",
        "\n",
        "      raw_sources_features_B[subj_row,:,:] = data_train_S_B\n",
        "      raw_sources_label_B[subj_row,:] = label_train_S_B\n",
        "\n",
        "      ######################### Target Session ##################################\n",
        "      fetures_Source_SESS_Target  = scipy.io.loadmat('/content/drive/MyDrive/SEED/ExtractedFeatures/' + path_Target_SESS[p-1])\n",
        "      feature_delta = []\n",
        "      feature_tetha = []\n",
        "      feature_alpha = []\n",
        "      feature_beta  = []\n",
        "      feature_gamma = []\n",
        "\n",
        "      field_name = []\n",
        "      for key in fetures_Source_SESS_Target.keys():\n",
        "          field_name.append(key)\n",
        "\n",
        "      label = []\n",
        "      for i in all_trials:\n",
        "        data_trial = fetures_Source_SESS_Target[field_name[12*i+num_feature]]\n",
        "        for k in range(140,data_trial.shape[1]):\n",
        "          data_epoch   = data_trial[:,k,:]\n",
        "          data_epoch   = np.squeeze(data_epoch)\n",
        "          a = []\n",
        "          b = []\n",
        "          c = []\n",
        "          d = []\n",
        "          e = []\n",
        "          for j in range(0,num_ch):\n",
        "            data_channel = data_epoch[sel_ch[j],:]\n",
        "            a.append(data_channel[0])\n",
        "            b.append(data_channel[1])\n",
        "            c.append(data_channel[2])\n",
        "            d.append(data_channel[3])\n",
        "            e.append(data_channel[4])\n",
        "\n",
        "          feature_delta.append(np.array(a))\n",
        "          feature_tetha.append(np.array(b))\n",
        "          feature_alpha.append(np.array(c))\n",
        "          feature_beta.append(np.array(d))\n",
        "          feature_gamma.append(np.array(e))\n",
        "          label.append(label_sess_1[0,i])\n",
        "\n",
        "      feature_delta = np.array(feature_delta)\n",
        "      feature_tetha = np.array(feature_tetha)\n",
        "      feature_alpha = np.array(feature_alpha)\n",
        "      feature_beta  = np.array(feature_beta)\n",
        "      feature_gamma = np.array(feature_gamma)\n",
        "      label        = np.array(label)\n",
        "      all_features = feature_gamma\n",
        "\n",
        "      num_sample = all_features.shape[0]\n",
        "      if init_Target==1:\n",
        "        raw_sources_features_Target = np.zeros((14,num_sample,num_ch))\n",
        "        raw_sources_label_Target = np.zeros((14,num_sample))\n",
        "        init_Target = 0\n",
        "\n",
        "      ## label and data for Target Session\n",
        "      data_train_S_Target  = all_features\n",
        "      label_train_S_Target = label\n",
        "\n",
        "      raw_sources_features_Target[subj_row,:,:] = data_train_S_Target\n",
        "      raw_sources_label_Target[subj_row,:] = label_train_S_Target\n",
        "      subj_row += 1\n",
        "\n",
        "    # Augmented sources data: align each session of each subject with the classifier's corresponding target session\n",
        "    Aggregated_Sessions_shape = 3*raw_sources_features_A.shape[1]\n",
        "    Augmented_sources_features = np.zeros((14,14,Aggregated_Sessions_shape,sub_dim))\n",
        "    TargetSess_Projected_Data_AllSources = np.zeros((14,raw_sources_features_Target.shape[1],sub_dim))\n",
        "    Augmented_Sources_label = np.zeros((14,14,Aggregated_Sessions_shape))\n",
        "    for source in range(14):\n",
        "      Source_Data_Sess_A = raw_sources_features_A[source,:,:]\n",
        "      Source_Data_Sess_B = raw_sources_features_B[source,:,:]\n",
        "      Source_Data_Sess_Target = raw_sources_features_Target[source,:,:]\n",
        "      TargetSession_Aligned_Source_Data_A, Target_Projected_Data, _ = subspace_alignment(Source_Data_Sess_A, Source_Data_Sess_Target, sub_dim)\n",
        "      TargetSession_Aligned_Source_Data_B, Target_Projected_Data, _ = subspace_alignment(Source_Data_Sess_B, Source_Data_Sess_Target, sub_dim)\n",
        "      TargetSess_Projected_Data_AllSources[source,:,:] = Target_Projected_Data\n",
        "      Augmented_sources_features[source,source,:,:] = np.concatenate((TargetSession_Aligned_Source_Data_A, TargetSession_Aligned_Source_Data_B, Target_Projected_Data),0)\n",
        "      Augmented_Sources_label[source,source,:] = np.concatenate((raw_sources_label_A[source,:], raw_sources_label_B[source,:], raw_sources_label_Target[source,:]),0)\n",
        "      other_sources = [*range(0,14)]\n",
        "      other_sources.remove(source)\n",
        "      for other_source in other_sources:\n",
        "        Other_Source_Data_Sess_A = raw_sources_features_A[other_source,:,:]\n",
        "        Other_Source_Data_Sess_B = raw_sources_features_B[other_source,:,:]\n",
        "        Other_Source_Data_Sess_Target = raw_sources_features_Target[other_source,:,:]\n",
        "        TargetSession_Aligned_Other_Source_Data_A, _, _ = subspace_alignment(Other_Source_Data_Sess_A, Source_Data_Sess_Target, sub_dim)\n",
        "        TargetSession_Aligned_Other_Source_Data_B, _, _ = subspace_alignment(Other_Source_Data_Sess_B, Source_Data_Sess_Target, sub_dim)\n",
        "        TargetSession_Aligned_Other_Source_Data_Target, _, _ = subspace_alignment(Other_Source_Data_Sess_Target, Source_Data_Sess_Target, sub_dim)\n",
        "        Augmented_sources_features[source,other_source,:,:] = np.concatenate((TargetSession_Aligned_Other_Source_Data_A, TargetSession_Aligned_Other_Source_Data_B, TargetSession_Aligned_Other_Source_Data_Target),0)\n",
        "        Augmented_Sources_label[source,other_source,:] = np.concatenate((raw_sources_label_A[other_source,:], raw_sources_label_B[other_source,:], raw_sources_label_Target[other_source,:]),0)\n",
        "\n",
        "\n",
        "    ############ 2.Training the Classifiers ############\n",
        "    # we will train have 14 classifiers here (5 classifiers in this version of the code)\n",
        "    # train a 4-layer MLP for each source subject based on the corresponding augmented source data\n",
        "    # we save the classifiers for next steps which use them for optimal cluster selection step\n",
        "\n",
        "    print(\"\\nTraining the Classifiers Step:\\n\")\n",
        "\n",
        "    for source in range (1,15):\n",
        "      # data preparing\n",
        "      clust = cluster_labels[source-1]\n",
        "      same_cluster = clusters_index[clust]\n",
        "      print(\"tarining the source case: \"+str(source-1)+\"\\n\")\n",
        "      #print(\"all sources in this cluster: \" +str(same_cluster))\n",
        "      x_train = Augmented_sources_features[source-1,:,:,:]\n",
        "      x_train = x_train[same_cluster,:,:]\n",
        "\n",
        "      # weighting the Target Session of each classifier corresponding source\n",
        "      if cluster_num_of_sources[clust]!=1:\n",
        "        weight = (3*((cluster_num_of_sources[clust])-1))-1\n",
        "      else:\n",
        "        weight = 1\n",
        "      #print(\"weight: \"+str(weight))\n",
        "      source_TargetSess_weighted_data = np.repeat(TargetSess_Projected_Data_AllSources[source-1,:,:],weight,axis=0)\n",
        "      source_TargetSess_weighted_labels = np.repeat(raw_sources_label_Target[source-1,:],weight)\n",
        "\n",
        "      #print(\"x_train.shape before reshape: \"+str(x_train.shape))\n",
        "      x_train = x_train.reshape((cluster_num_of_sources[clust])*Aggregated_Sessions_shape,sub_dim)\n",
        "      #print(\"x_train.shape after reshape: \"+str(x_train.shape))\n",
        "\n",
        "      x_train = np.concatenate((x_train, source_TargetSess_weighted_data),0)\n",
        "\n",
        "      #print(\"x_train.shape after concatenating with weighted source data: \"+str(x_train.shape))\n",
        "\n",
        "      # label preparing\n",
        "      y_train = Augmented_Sources_label[source-1,:,:]\n",
        "      y_train = y_train[same_cluster,:]\n",
        "      y_train = y_train.reshape(1,-1)\n",
        "      y_train = np.squeeze(y_train)\n",
        "      #print(\"y_train.shape before concatenating with weighted source labels: \"+str(y_train.shape))\n",
        "\n",
        "      y_train = np.concatenate((y_train, source_TargetSess_weighted_labels),0)\n",
        "      #print(\"y_train.shape after concatenating with weighted source labels: \"+str(y_train.shape))\n",
        "\n",
        "      # Shuffle the Dataset and One-Hot the labels\n",
        "      x_train , y_train = shuffle(x_train, y_train,random_state=SEED_num)\n",
        "      y_train = tf.keras.utils.to_categorical(np.array(y_train),3)\n",
        "\n",
        "      initializer = tf.keras.initializers.GlorotNormal(seed=24)\n",
        "\n",
        "      model = Sequential([\n",
        "          Flatten(input_shape=(sub_dim,)),\n",
        "          Dense(32, activation= 'tanh', kernel_initializer=initializer, name='Dense_1'),\n",
        "          Dense(20, activation='tanh', kernel_initializer=initializer, name='Dense_2'),\n",
        "          Dense(10, activation='tanh', kernel_initializer=initializer, name='Dense_3'),\n",
        "          Dense(3, activation='softmax', kernel_initializer=initializer, name='output')\n",
        "      ])\n",
        "\n",
        "      model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      tf.random.set_seed(123)\n",
        "      history = model.fit(x_train, y_train, epochs = 20, batch_size= 256 ,shuffle = False, verbose=0)\n",
        "\n",
        "      model.save('/content/drive/MyDrive/SubjectClustering_SEED_AllSessions_MyVersion/classifiers/'+str(target_sub)+'-'+ str(source) +'-'+str(iter+1))\n",
        "\n",
        "      # plotting the accuracy and loss curves (if needed, otherwise SKIP them)\n",
        "\n",
        "      #plt.plot(history.history['loss'])\n",
        "      #plt.plot(history.history['val_loss'])\n",
        "      #plt.title(\"Loss Vs Epochs [case: \"+str(source)+\"][iteration: \"+str(iter+1)+\"]\")\n",
        "      #plt.ylabel('Loss')\n",
        "      #plt.xlabel('Epochs')\n",
        "      #plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
        "      #plt.show()\n",
        "\n",
        "      #plt.plot(history.history['accuracy'])\n",
        "      #plt.plot(history.history['val_accuracy'])\n",
        "      #plt.title(\"Accuracy Vs Epochs [case: \"+str(source)+\"][iteration: \"+str(iter+1)+\"]\")\n",
        "      #plt.ylabel('Accuracy')\n",
        "      #plt.xlabel('Epochs')\n",
        "      #plt.legend(['Training Accuracy', 'Validation Accuracy'], loc='lower right')\n",
        "      #plt.show()\n",
        "\n",
        "    ############ 3. Cluster Selection ############\n",
        "\n",
        "\n",
        "    ################# Target Training Data #################\n",
        "\n",
        "    ########## Target training data trials (Session A) ##########\n",
        "    features_Target_A = scipy.io.loadmat('/content/drive/MyDrive/SEED/ExtractedFeatures/' + path_Source_SESS_A[target_sub-1])\n",
        "\n",
        "    feature_delta = []\n",
        "    feature_tetha = []\n",
        "    feature_alpha = []\n",
        "    feature_beta  = []\n",
        "    feature_gamma = []\n",
        "\n",
        "    field_name = []\n",
        "    for key in features_Target_A.keys():\n",
        "        field_name.append(key)\n",
        "\n",
        "    label = []\n",
        "    for i in all_trials:\n",
        "      data_trial = features_Target_A[field_name[12*i+num_feature]]\n",
        "      for k in range(140,data_trial.shape[1]):\n",
        "        data_epoch   = data_trial[:,k,:]\n",
        "        data_epoch   = np.squeeze(data_epoch)\n",
        "        a = []\n",
        "        b = []\n",
        "        c = []\n",
        "        d = []\n",
        "        e = []\n",
        "        for j in range(0,num_ch):\n",
        "          data_channel = data_epoch[sel_ch[j],:]\n",
        "          a.append(data_channel[0])\n",
        "          b.append(data_channel[1])\n",
        "          c.append(data_channel[2])\n",
        "          d.append(data_channel[3])\n",
        "          e.append(data_channel[4])\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        c = np.array(c)\n",
        "        d = np.array(d)\n",
        "        e = np.array(e)\n",
        "\n",
        "        feature_delta.append(a)\n",
        "        feature_tetha.append(b)\n",
        "        feature_alpha.append(c)\n",
        "        feature_beta.append(d)\n",
        "        feature_gamma.append(e)\n",
        "        label.append(label_sess_1[0,i])\n",
        "    feature_delta = np.array(feature_delta)\n",
        "    feature_tetha = np.array(feature_tetha)\n",
        "    feature_alpha = np.array(feature_alpha)\n",
        "    feature_beta  = np.array(feature_beta)\n",
        "    feature_gamma = np.array(feature_gamma)\n",
        "    label        = np.array(label)\n",
        "    all_features = feature_gamma\n",
        "\n",
        "    ## label and data for Target Subject's Source Session A set\n",
        "    data_train_T_Sess_A  = all_features\n",
        "    label_train_T_Sess_A = label\n",
        "\n",
        "    ########## Target training data trials (Session B) ##########\n",
        "    features_Target_Sess_B = scipy.io.loadmat('/content/drive/MyDrive/SEED/ExtractedFeatures/' + path_Source_SESS_B[target_sub-1])\n",
        "\n",
        "    feature_delta = []\n",
        "    feature_tetha = []\n",
        "    feature_alpha = []\n",
        "    feature_beta  = []\n",
        "    feature_gamma = []\n",
        "\n",
        "    field_name = []\n",
        "    for key in features_Target_Sess_B.keys():\n",
        "        field_name.append(key)\n",
        "\n",
        "    label = []\n",
        "    for i in all_trials:\n",
        "      data_trial = features_Target_Sess_B[field_name[12*i+num_feature]]\n",
        "      for k in range(140,data_trial.shape[1]):\n",
        "        data_epoch   = data_trial[:,k,:]\n",
        "        data_epoch   = np.squeeze(data_epoch)\n",
        "        a = []\n",
        "        b = []\n",
        "        c = []\n",
        "        d = []\n",
        "        e = []\n",
        "        for j in range(0,num_ch):\n",
        "          data_channel = data_epoch[sel_ch[j],:]\n",
        "          a.append(data_channel[0])\n",
        "          b.append(data_channel[1])\n",
        "          c.append(data_channel[2])\n",
        "          d.append(data_channel[3])\n",
        "          e.append(data_channel[4])\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        c = np.array(c)\n",
        "        d = np.array(d)\n",
        "        e = np.array(e)\n",
        "\n",
        "        feature_delta.append(a)\n",
        "        feature_tetha.append(b)\n",
        "        feature_alpha.append(c)\n",
        "        feature_beta.append(d)\n",
        "        feature_gamma.append(e)\n",
        "        label.append(label_sess_1[0,i])\n",
        "    feature_delta = np.array(feature_delta)\n",
        "    feature_tetha = np.array(feature_tetha)\n",
        "    feature_alpha = np.array(feature_alpha)\n",
        "    feature_beta  = np.array(feature_beta)\n",
        "    feature_gamma = np.array(feature_gamma)\n",
        "    label        = np.array(label)\n",
        "    all_features = feature_gamma\n",
        "\n",
        "    ## label and data for Target Subject's Source Session B set\n",
        "    data_train_T_Sess_B  = all_features\n",
        "    label_train_T_Sess_B = label\n",
        "\n",
        "\n",
        "    ########## Target training data trials (Target Session) ##########\n",
        "    features_Test = scipy.io.loadmat('/content/drive/MyDrive/SEED/ExtractedFeatures/' + path_Target_SESS[target_sub-1])\n",
        "    feature_delta = []\n",
        "    feature_tetha = []\n",
        "    feature_alpha = []\n",
        "    feature_beta  = []\n",
        "    feature_gamma = []\n",
        "\n",
        "    field_name = []\n",
        "    for key in features_Test.keys():\n",
        "        field_name.append(key)\n",
        "\n",
        "    label = []\n",
        "    for i in train_trials:\n",
        "      data_trial = features_Test[field_name[12*i+num_feature]]\n",
        "      for k in range(140,data_trial.shape[1]):\n",
        "        data_epoch   = data_trial[:,k,:]\n",
        "        data_epoch   = np.squeeze(data_epoch)\n",
        "        a = []\n",
        "        b = []\n",
        "        c = []\n",
        "        d = []\n",
        "        e = []\n",
        "        for j in range(0,num_ch):\n",
        "          data_channel = data_epoch[sel_ch[j],:]\n",
        "          a.append(data_channel[0])\n",
        "          b.append(data_channel[1])\n",
        "          c.append(data_channel[2])\n",
        "          d.append(data_channel[3])\n",
        "          e.append(data_channel[4])\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        c = np.array(c)\n",
        "        d = np.array(d)\n",
        "        e = np.array(e)\n",
        "\n",
        "        feature_delta.append(a)\n",
        "        feature_tetha.append(b)\n",
        "        feature_alpha.append(c)\n",
        "        feature_beta.append(d)\n",
        "        feature_gamma.append(e)\n",
        "        label.append(label_sess_1[0,i])\n",
        "    feature_delta = np.array(feature_delta)\n",
        "    feature_tetha = np.array(feature_tetha)\n",
        "    feature_alpha = np.array(feature_alpha)\n",
        "    feature_beta  = np.array(feature_beta)\n",
        "    feature_gamma = np.array(feature_gamma)\n",
        "    label        = np.array(label)\n",
        "    all_features = feature_gamma\n",
        "\n",
        "    ## label and data for Target Subject's Target Session Set\n",
        "    data_train_T  = all_features\n",
        "    label_train_T = label\n",
        "\n",
        "    ################# Target test data trials (from Target Session) #################\n",
        "    feature_delta = []\n",
        "    feature_tetha = []\n",
        "    feature_alpha = []\n",
        "    feature_beta  = []\n",
        "    feature_gamma = []\n",
        "\n",
        "    label = []\n",
        "    for i in valid_trials:\n",
        "      data_trial = features_Test[field_name[12*i+num_feature]]\n",
        "      for k in range(140,data_trial.shape[1]):\n",
        "        data_epoch   = data_trial[:,k,:]\n",
        "        data_epoch   = np.squeeze(data_epoch)\n",
        "        a = []\n",
        "        b = []\n",
        "        c = []\n",
        "        d = []\n",
        "        e = []\n",
        "        for j in range(0,num_ch):\n",
        "          data_channel = data_epoch[sel_ch[j],:]\n",
        "          a.append(data_channel[0])\n",
        "          b.append(data_channel[1])\n",
        "          c.append(data_channel[2])\n",
        "          d.append(data_channel[3])\n",
        "          e.append(data_channel[4])\n",
        "        a = np.array(a)\n",
        "        b = np.array(b)\n",
        "        c = np.array(c)\n",
        "        d = np.array(d)\n",
        "        e = np.array(e)\n",
        "\n",
        "        feature_delta.append(a)\n",
        "        feature_tetha.append(b)\n",
        "        feature_alpha.append(c)\n",
        "        feature_beta.append(d)\n",
        "        feature_gamma.append(e)\n",
        "        label.append(label_sess_1[0,i])\n",
        "    feature_delta = np.array(feature_delta)\n",
        "    feature_tetha = np.array(feature_tetha)\n",
        "    feature_alpha = np.array(feature_alpha)\n",
        "    feature_beta  = np.array(feature_beta)\n",
        "    feature_gamma = np.array(feature_gamma)\n",
        "    label        = np.array(label)\n",
        "    all_features = feature_gamma\n",
        "\n",
        "    ## test labels and data from target subject's Target Session\n",
        "    data_valid  = all_features\n",
        "    label_valid = label\n",
        "\n",
        "    # Projected Target (training) Data\n",
        "    num_sample_T = data_train_T.shape[0]\n",
        "    Projected_Target_Data = np.zeros((14,num_sample_T,sub_dim))\n",
        "    for source_row in range(0,14):\n",
        "      Source_Data = raw_sources_features_Target[source_row,:,:]\n",
        "      Target_Data = data_train_T\n",
        "      Projected_Target_Data[source_row,:,:], _, _ = subspace_alignment(Target_Data, Source_Data, sub_dim)\n",
        "\n",
        "    # Projected Target (test) Data\n",
        "    num_sample_test = data_valid.shape[0]\n",
        "    Projected_Target_Data_test = np.zeros((14,num_sample_test,sub_dim))\n",
        "    for source_row in range(0,14):\n",
        "      Source_Data = raw_sources_features_Target[source_row,:,:]\n",
        "      Target_Data = data_valid\n",
        "      Projected_Target_Data_test[source_row,:,:], _, _ = subspace_alignment(Target_Data, Source_Data, sub_dim)\n",
        "\n",
        "    # prediction matrix (for train)\n",
        "    Prediction_Matrix = np.zeros((14,num_sample_T))\n",
        "    Prediction_Probs = np.zeros((14,num_sample_T,3))\n",
        "    for classifier in range(1,15):\n",
        "      # load model\n",
        "      model = tf.keras.models.load_model('/content/drive/MyDrive/SubjectClustering_SEED_AllSessions_MyVersion/classifiers/'+str(target_sub)+'-'+ str(classifier) +'-'+str(iter+1))\n",
        "\n",
        "      # predict label for the samples\n",
        "      input_samples = Projected_Target_Data[classifier-1,:,:]\n",
        "      labels = model.predict(input_samples)\n",
        "      Prediction_Matrix[classifier-1,:] = np.argmax(np.squeeze(model.predict(input_samples)),axis=1)\n",
        "      Prediction_Probs[classifier-1,:,:] = np.squeeze(model.predict(input_samples))\n",
        "\n",
        "      prediction_matrix_non_onehot = np.zeros((14,num_sample_T))\n",
        "      for source_row in range(14):\n",
        "        for sample in range(num_sample_T):\n",
        "          if Prediction_Matrix[source_row,sample]==0:\n",
        "            prediction_matrix_non_onehot[source_row,sample]=0\n",
        "          elif Prediction_Matrix[source_row,sample]==1:\n",
        "            prediction_matrix_non_onehot[source_row,sample]=1\n",
        "          elif Prediction_Matrix[source_row,sample]==2:\n",
        "            prediction_matrix_non_onehot[source_row,sample]=-1\n",
        "\n",
        "    # Prediction matrix (for test)\n",
        "    Prediction_Matrix_test = np.zeros((14,num_sample_test))\n",
        "    Prediction_Probs_test = np.zeros((14,num_sample_test,3))\n",
        "    for classifier in range(1,15):\n",
        "      # load model\n",
        "      model = tf.keras.models.load_model('/content/drive/MyDrive/SubjectClustering_SEED_AllSessions_MyVersion/classifiers/'+str(target_sub)+'-'+ str(classifier) +'-'+str(iter+1))\n",
        "\n",
        "      # predict label for the samples\n",
        "      input_samples = Projected_Target_Data_test[classifier-1,:,:]\n",
        "      labels = model.predict(input_samples)\n",
        "      Prediction_Matrix_test[classifier-1,:] = np.argmax(np.squeeze(model.predict(input_samples)),axis=1)\n",
        "      Prediction_Probs_test[classifier-1,:,:] = np.squeeze(model.predict(input_samples))\n",
        "\n",
        "    prediction_matrix_test_non_onehot = np.zeros((14,num_sample_test))\n",
        "    for source_row in range(14):\n",
        "      for sample in range(num_sample_test):\n",
        "        if Prediction_Matrix_test[source_row,sample]==0:\n",
        "          prediction_matrix_test_non_onehot[source_row,sample]=0\n",
        "        elif Prediction_Matrix_test[source_row,sample]==1:\n",
        "          prediction_matrix_test_non_onehot[source_row,sample]=1\n",
        "        elif Prediction_Matrix_test[source_row,sample]==2:\n",
        "          prediction_matrix_test_non_onehot[source_row,sample]=-1\n",
        "\n",
        "    # ********* Supervised Optimal Cluster Selection Metrics *********\n",
        "\n",
        "    #MMPP: Maximum Major Prediction Proportion\n",
        "    # Optimal cluster selection (based on unlabled test trials)\n",
        "    clusters_proportions = np.zeros((best_num_cluster))\n",
        "    for cluster in range(0,best_num_cluster):\n",
        "      proportion_array = np.zeros((num_sample_test))\n",
        "      for sample in range(0,num_sample_test):\n",
        "        sum_prob_0 = 0\n",
        "        sum_prob_1 = 0\n",
        "        sum_prob_min1 = 0\n",
        "        counter_0 = 0\n",
        "        counter_1 = 0\n",
        "        counter_min1 = 0\n",
        "        use_prob = 0\n",
        "        for classifier in range(cluster_num_of_sources[cluster]):\n",
        "          source_index = clusters_source_index[cluster][classifier]\n",
        "          if prediction_matrix_test_non_onehot[source_index,sample]==0:\n",
        "            counter_0 += 1\n",
        "          elif prediction_matrix_test_non_onehot[source_index,sample]==1:\n",
        "            counter_1 += 1\n",
        "          elif prediction_matrix_test_non_onehot[source_index,sample]==-1:\n",
        "            counter_min1 += 1\n",
        "        if (((counter_0 == counter_1) and (counter_min1<counter_0)) or ((counter_0 == counter_min1)and(counter_1<counter_0)) or ((counter_1 == counter_min1)and(counter_0<counter_1)) or (counter_0 == counter_1 and counter_0 == counter_min1)):\n",
        "          for classifier in range(cluster_num_of_sources[cluster]):\n",
        "            sum_prob_0 += Prediction_Probs_test[source_index,sample,0]\n",
        "            sum_prob_1 += Prediction_Probs_test[source_index,sample,1]\n",
        "            sum_prob_min1 += Prediction_Probs_test[source_index,sample,2]\n",
        "          use_prob = 1\n",
        "        if use_prob==0:\n",
        "          major_pred = max(counter_0,counter_1,counter_min1)\n",
        "          proportion = major_pred/(cluster_num_of_sources[cluster])\n",
        "        else:\n",
        "          sum_array = np.array([sum_prob_0,sum_prob_1,sum_prob_min1])\n",
        "          counter_array = np.array([counter_0,counter_1,counter_min1])\n",
        "          major_pred = np.argmax(sum_array)\n",
        "          proportion = counter_array[major_pred]/(cluster_num_of_sources[cluster])\n",
        "        proportion_array[sample] = proportion\n",
        "      Avg_proportion = np.mean(proportion_array)\n",
        "      clusters_proportions[cluster] = Avg_proportion\n",
        "    print(\"\\n\\nclusters proportions: \"+str(clusters_proportions))\n",
        "    K_Selected_MMPP = np.argmax(clusters_proportions)\n",
        "    print('K (Selected Cluster)(MMPP): '+str(K_Selected_MMPP +1))\n",
        "\n",
        "    #MCPCP: Maximum Correctly Predicted Classifiers Proportion\n",
        "    # Optimal cluster selection (based on available Target training samples)\n",
        "    clusters_proportions = np.zeros((best_num_cluster))\n",
        "    for cluster in range(0,best_num_cluster):\n",
        "      proportion_array = np.zeros((num_sample_T))\n",
        "      for sample in range(0,num_sample_T):\n",
        "        counter_correct = 0\n",
        "        for classifier in range(cluster_num_of_sources[cluster]):\n",
        "          source_index = clusters_source_index[cluster][classifier]\n",
        "          if prediction_matrix_non_onehot[source_index,sample]==label_train_T[sample]:\n",
        "            counter_correct += 1\n",
        "        proportion_array[sample] = counter_correct/cluster_num_of_sources[cluster]\n",
        "      clusters_proportions[cluster] = np.mean(proportion_array)\n",
        "    print(\"\\n\\nclusters proportions: \"+str(clusters_proportions))\n",
        "    K_Selected_MNCC = np.argmax(clusters_proportions)\n",
        "    print('K (Selected Cluster)(MNCC): '+str(K_Selected_MNCC +1))\n",
        "\n",
        "    #Maximum Mean Accuracy (MMA) and Maximum Top Accuracy (MTA)\n",
        "    # Optimal cluster selection (based on available Target training samples)\n",
        "    mean_clusters_acc = np.zeros((best_num_cluster))\n",
        "    high_clusters_acc = np.zeros((best_num_cluster))\n",
        "    for cluster in range(best_num_cluster):\n",
        "      classifiers_acc = np.zeros((cluster_num_of_sources[cluster]))\n",
        "      for classifier in range(cluster_num_of_sources[cluster]):\n",
        "        source_index = clusters_source_index[cluster][classifier]\n",
        "        acc = (sum(np.equal(label_train_T,prediction_matrix_non_onehot[source_index,:][0:len(label_train_T)])))/len(label_train_T)\n",
        "        classifiers_acc[classifier] = acc\n",
        "      mean_clusters_acc[cluster] = np.mean(classifiers_acc)\n",
        "      high_clusters_acc[cluster] = np.max(classifiers_acc)\n",
        "    print(\"\\n\\nmean accuracy for classifiers in each cluster: \"+str(mean_clusters_acc))\n",
        "    print(\"high accuracy for classifiers in each cluster: \"+str(high_clusters_acc))\n",
        "    K_Selected_MMA = np.argmax(mean_clusters_acc)\n",
        "    K_Selected_MTA = np.argmax(high_clusters_acc)\n",
        "    print('K (MMA): '+str(K_Selected_MMA+1))\n",
        "    print('K (MTA): '+str(K_Selected_MTA+1))\n",
        "    print(\"\\n\\nK (Clustering): \"+str(K_clustering+1)+\"\\n\")\n",
        "\n",
        "    ############ 4. Source Selection (Optional Step) ############\n",
        "\n",
        "    final_accuracy =[]\n",
        "    High_acc_in_iter = 0\n",
        "    for k in range(best_num_cluster):\n",
        "    #for k in [K_Selected,K_clustering]:\n",
        "    #for k in [K_Selected]:\n",
        "      K=k\n",
        "      #################  train Booster Classifier on Target Subject's All Sessions Data   #################\n",
        "      Booster_Data_Shape = 3*data_train_T_Sess_A.shape[0]\n",
        "      Booster_TargetSess_Alinged_Sess_A, Booster_TargetSess_Projected,_ = subspace_alignment(data_train_T_Sess_A, data_train_T, sub_dim)\n",
        "      Booster_TragetSess_Aligned_Sess_B,_,_ = subspace_alignment(data_train_T_Sess_B, data_train_T, sub_dim)\n",
        "      Booster_Data = np.concatenate((Booster_TargetSess_Alinged_Sess_A, Booster_TragetSess_Aligned_Sess_B, Booster_TargetSess_Projected),0)\n",
        "      Booster_label = np.concatenate((label_train_T_Sess_A, label_train_T_Sess_B, label_train_T),0)\n",
        "\n",
        "      Booster_weight = (3*(cluster_num_of_sources[K]))-1\n",
        "      Booster_TargetSess_Weighted_Data = np.repeat(Booster_TargetSess_Projected,Booster_weight,axis=0)\n",
        "      Booster_TargetSess_Weighted_label = np.repeat(label_train_T,Booster_weight)\n",
        "\n",
        "      other_sources_aggregated_shape = 3*raw_sources_features_A.shape[1]\n",
        "      Booster_Selected_cluster_data = np.zeros((cluster_num_of_sources[K],other_sources_aggregated_shape,sub_dim))\n",
        "      Booster_Selected_cluster_label = np.zeros((cluster_num_of_sources[K],other_sources_aggregated_shape))\n",
        "      for source in range(cluster_num_of_sources[K]):\n",
        "        source_index = clusters_source_index[K][source]\n",
        "        Other_Source_Data_Sess_A = raw_sources_features_A[source_index,:,:]\n",
        "        Other_Source_Data_Sess_B = raw_sources_features_B[source_index,:,:]\n",
        "        Other_Source_Data_Sess_Target = raw_sources_features_Target[source_index,:,:]\n",
        "        Booster_TargetSess_Aligned_Other_Source_Data_A, _, _ = subspace_alignment(Other_Source_Data_Sess_A, data_train_T, sub_dim)\n",
        "        Booster_TargetSess_Aligned_Other_Source_Data_B, _, _ = subspace_alignment(Other_Source_Data_Sess_B, data_train_T, sub_dim)\n",
        "        Booster_TargetSess_Aligned_Other_Source_Data_TargetSess, _, _ = subspace_alignment(Other_Source_Data_Sess_Target, data_train_T, sub_dim)\n",
        "        Booster_TargetSess_Aligned_other_Source_Data = np.concatenate((Booster_TargetSess_Aligned_Other_Source_Data_A, Booster_TargetSess_Aligned_Other_Source_Data_B, Booster_TargetSess_Aligned_Other_Source_Data_TargetSess),0)\n",
        "        Booster_TargetSess_Aligned_other_Source_label = np.concatenate((raw_sources_label_A[source_index,:], raw_sources_label_B[source_index,:], raw_sources_label_Target[source_index,:]),0)\n",
        "        Booster_Selected_cluster_data[source,:,:] = Booster_TargetSess_Aligned_other_Source_Data\n",
        "        Booster_Selected_cluster_label[source,:] = Booster_TargetSess_Aligned_other_Source_label\n",
        "      Booster_Selected_cluster_data = Booster_Selected_cluster_data.reshape(cluster_num_of_sources[K]*other_sources_aggregated_shape,sub_dim)\n",
        "      Booster_Selected_cluster_label = Booster_Selected_cluster_label.reshape(-1,1)\n",
        "      Booster_Selected_cluster_label = np.squeeze(Booster_Selected_cluster_label)\n",
        "\n",
        "      Booster_x_train = np.concatenate((Booster_Data,Booster_TargetSess_Weighted_Data,Booster_Selected_cluster_data),0)\n",
        "      Booster_y_train = np.concatenate((Booster_label,Booster_TargetSess_Weighted_label,Booster_Selected_cluster_label),0)\n",
        "\n",
        "      Booster_x_train , Booster_y_train = shuffle(Booster_x_train, Booster_y_train,random_state=SEED_num)\n",
        "      Booster_y_train = tf.keras.utils.to_categorical(np.array(Booster_y_train),3)\n",
        "      model_booster = Sequential([\n",
        "            Flatten(input_shape=(sub_dim,)),\n",
        "            Dense(32, activation= 'tanh', kernel_initializer=initializer, name='Dense_1'),\n",
        "            Dense(20, activation='tanh', kernel_initializer=initializer, name='Dense_2'),\n",
        "            Dense(10, activation='tanh', kernel_initializer=initializer, name='Dense_3'),\n",
        "            Dense(3, activation='softmax', kernel_initializer=initializer, name='output')\n",
        "        ])\n",
        "      print(\"\\ntarining the Booster Classifier for this cluster!\\n\")\n",
        "      model_booster.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      tf.random.set_seed(123)\n",
        "      history = model_booster.fit(Booster_x_train, Booster_y_train, epochs = 5, batch_size= 256, shuffle = False, verbose=1)\n",
        "      model_booster.save('/content/drive/MyDrive/SubjectClustering_SEED_AllSessions_MyVersion/classifiers/'+str(target_sub)+'-Booster-'+str(iter+1))\n",
        "\n",
        "      CLUSTER_ACCURACY = np.zeros((cluster_num_of_sources[K]+1))\n",
        "      cluster_k_accuracy = np.zeros((cluster_num_of_sources[K]))\n",
        "      for classifier in range(cluster_num_of_sources[K]):\n",
        "        classifier_index = clusters_source_index[K][classifier]\n",
        "        pred_labels = prediction_matrix_non_onehot[classifier_index,:]\n",
        "        ground_truth = label_train_T\n",
        "        acc = (sum(np.equal(ground_truth,pred_labels[0:len(ground_truth)])))/len(ground_truth)\n",
        "        cluster_k_accuracy[classifier] = acc\n",
        "      #print(\"classifiers accuracy for selected cluster: \"+str(cluster_k_accuracy))\n",
        "\n",
        "      # Selecting top N classifiers with high accuracies\n",
        "      #N = cluster_num_of_sources[K] if (cluster_num_of_sources[K]%2==1) else cluster_num_of_sources[K]-1\n",
        "      #N = cluster_num_of_sources[K]\n",
        "      #N = 3\n",
        "      N_counter = 0\n",
        "      for N in range(0,cluster_num_of_sources[K]+1):\n",
        "      #for N in [0]:\n",
        "        if N>0:\n",
        "          sorted_indices = np.argsort(cluster_k_accuracy)\n",
        "          idx = sorted_indices[::-1][:N]\n",
        "          sorted_classifiers = []\n",
        "          for i in range(N):\n",
        "            sorted_classifiers.append(clusters_source_index[K][idx[i]])\n",
        "          #print(\"selected source indices: \"+str(sorted_classifiers))\n",
        "\n",
        "          #align Target (test samples) with selected sources\n",
        "          Aligned_Target = np.zeros((N,data_valid.shape[0],sub_dim))\n",
        "          for source in range(N):\n",
        "            source_index = sorted_classifiers[source]\n",
        "            Selected_Source_Data = raw_sources_features_Target[source_index,:,:]\n",
        "            Target_Data = data_valid\n",
        "            Aligned_Target[source,:,:], _, _ = subspace_alignment(Target_Data, Selected_Source_Data , sub_dim)\n",
        "\n",
        "        # align Traget (test samples) with Target training sample (Booster Classifier)\n",
        "        Aligned_Target_Booster,_,_ = subspace_alignment(data_valid, data_train_T , sub_dim)\n",
        "\n",
        "        # load models\n",
        "        preds_matrix= np.zeros((N+1,data_valid.shape[0],3))\n",
        "        selected_sources_accuracy = np.zeros((N+1))\n",
        "        prediction = np.zeros(data_valid.shape[0])\n",
        "\n",
        "        # Booster\n",
        "        Booster_model = tf.keras.models.load_model('/content/drive/MyDrive/SubjectClustering_SEED_AllSessions_MyVersion/classifiers/'+str(target_sub)+'-Booster-'+str(iter+1))\n",
        "        input_samples = Aligned_Target_Booster\n",
        "        label_prob = Booster_model.predict(input_samples)\n",
        "        Booster_predicited_label = np.argmax(np.squeeze(Booster_model.predict(input_samples)),axis=1)\n",
        "        Booster_pred_labels_prob = np.squeeze(Booster_model.predict(input_samples))\n",
        "        preds_matrix[0,:,:] = Booster_pred_labels_prob\n",
        "        Booster_pred_labels_non_onehot = np.zeros((data_valid.shape[0]))\n",
        "        for sample in range(data_valid.shape[0]):\n",
        "          if Booster_predicited_label[sample]==0:\n",
        "            Booster_pred_labels_non_onehot[sample]=0\n",
        "          elif Booster_predicited_label[sample]==1:\n",
        "            Booster_pred_labels_non_onehot[sample]=1\n",
        "          elif Booster_predicited_label[sample]==2:\n",
        "            Booster_pred_labels_non_onehot[sample]=-1\n",
        "        Booster_accuracy = (sum(np.equal(label_valid,Booster_pred_labels_non_onehot[0:len(label_valid)])))/len(label_valid)\n",
        "        selected_sources_accuracy[0] = Booster_accuracy\n",
        "\n",
        "        if N>0:\n",
        "          for classifier_num in range(N):\n",
        "            classifier_index = sorted_classifiers[classifier_num]\n",
        "            model = tf.keras.models.load_model('/content/drive/MyDrive/SubjectClustering_SEED_AllSessions_MyVersion/classifiers/'+str(target_sub)+'-'+ str(classifier_index+1) +'-'+str(iter+1))\n",
        "            input_samples = Aligned_Target[classifier_num,:,:]\n",
        "            label_prob = model.predict(input_samples)\n",
        "            classifier_predicited_label = np.argmax(np.squeeze(model.predict(input_samples)),axis=1)\n",
        "            classifier_pred_labels_prob = np.squeeze(model.predict(input_samples))\n",
        "            preds_matrix[classifier_num+1,:,:] = classifier_pred_labels_prob\n",
        "\n",
        "            classifier_pred_labels_non_onehot = np.zeros((data_valid.shape[0]))\n",
        "            for sample in range(data_valid.shape[0]):\n",
        "              if classifier_predicited_label[sample]==0:\n",
        "                classifier_pred_labels_non_onehot[sample]=0\n",
        "              elif classifier_predicited_label[sample]==1:\n",
        "                classifier_pred_labels_non_onehot[sample]=1\n",
        "              elif classifier_predicited_label[sample]==2:\n",
        "                classifier_pred_labels_non_onehot[sample]=-1\n",
        "            #print(label_valid)\n",
        "            #print(classifier_pred_labels_non_onehot)\n",
        "            classifier_accuracy = (sum(np.equal(label_valid,classifier_pred_labels_non_onehot[0:len(label_valid)])))/len(label_valid)\n",
        "            selected_sources_accuracy[classifier_num+1] = classifier_accuracy\n",
        "          #print(\"\\naccuracy (on test samples) for classifiers in this cluster :\")\n",
        "          #print(selected_sources_accuracy)\n",
        "          #print(\"\\n\")\n",
        "\n",
        "        # calculate major prediction (soft voting)\n",
        "        for sample in range(data_valid.shape[0]):\n",
        "          sum_prob_0 = 0\n",
        "          sum_prob_1 = 0\n",
        "          sum_prob_min1 = 0\n",
        "          for classifier in range(N+1):\n",
        "            sum_prob_0 += preds_matrix[classifier,sample,0]\n",
        "            sum_prob_1 += preds_matrix[classifier,sample,1]\n",
        "            sum_prob_min1 += preds_matrix[classifier,sample,2]\n",
        "          #print(\"for sample \"+str(sample))\n",
        "          #print(\"sum_prob_0: \"+str(sum_prob_0))\n",
        "          #print(\"sum_prob_1: \"+str(sum_prob_1))\n",
        "          #print(\"sum_prob_min1: \"+str(sum_prob_min1))\n",
        "          sample_label = 0\n",
        "          if (sum_prob_0 > sum_prob_1 and sum_prob_0 > sum_prob_min1):\n",
        "            sample_label = 0\n",
        "          elif (sum_prob_1 > sum_prob_0 and sum_prob_1 > sum_prob_min1):\n",
        "            sample_label = 1\n",
        "          elif (sum_prob_min1 > sum_prob_0 and sum_prob_min1 > sum_prob_1):\n",
        "            sample_label = -1\n",
        "          prediction[sample] = sample_label\n",
        "          #print(\"predicted label: \"+str(sample_label)+\"\\n\")\n",
        "\n",
        "        # compute the accuracy for target test samples\n",
        "        acc_iter = (sum(np.equal(label_valid,prediction[0:len(label_valid)])))/len(label_valid)\n",
        "        CLUSTER_ACCURACY[N_counter] = 100*acc_iter\n",
        "        N_counter += 1\n",
        "        print('\\naccuracy for cluster '+str(K+1)+\" with N = \"+str(N)+\" in this iteration : \"+str(acc_iter))\n",
        "      if CLUSTER_ACCURACY[0]>High_acc_in_iter:\n",
        "        High_acc_in_iter = CLUSTER_ACCURACY[0]\n",
        "      print(\"\\n\")\n",
        "      plt.title(\"Accuracy Vs N (Number of Selected Classifiers) [Cluster: \"+str(K+1)+\" of \"+str(best_num_cluster)+\"][iteration: \"+str(iter+1)+\" of 5]\")\n",
        "      plt.ylabel('Accuracy (%)')\n",
        "      plt.xlabel('N (Number of Selected Classifiers)')\n",
        "      plt.xticks(np.arange(N+1), np.arange(0, N+2))\n",
        "      plt.plot(CLUSTER_ACCURACY, color=\"blue\", marker = '*')\n",
        "      plt.show()\n",
        "      if K==K_Selected_MMPP:\n",
        "        iter_acc_list_selected_MMPP.append(CLUSTER_ACCURACY)\n",
        "      if K==K_clustering:\n",
        "        iter_acc_list_clustering.append(CLUSTER_ACCURACY)\n",
        "        #print(\"iter_acc_list\")\n",
        "        #print(iter_acc_list)\n",
        "      if K==K_Selected_MNCC:\n",
        "        iter_acc_list_selected_MNCC.append(CLUSTER_ACCURACY)\n",
        "      if K==K_Selected_MMA:\n",
        "        iter_acc_list_selected_MMA.append(CLUSTER_ACCURACY)\n",
        "      if K==K_Selected_MTA:\n",
        "        iter_acc_list_selected_MTA.append(CLUSTER_ACCURACY)\n",
        "    iter_acc_list_HIGHEST.append(High_acc_in_iter)\n",
        "    print(\"Potential Highest Acc: \"+str(iter_acc_list_HIGHEST))\n",
        "  Target_acc_list_selected_MMPP.append(iter_acc_list_selected_MMPP)\n",
        "  Target_acc_list_selected_MNCC.append(iter_acc_list_selected_MNCC)\n",
        "  Target_acc_list_selected_MMA.append(iter_acc_list_selected_MMA)\n",
        "  Target_acc_list_selected_MTA.append(iter_acc_list_selected_MTA)\n",
        "  Target_acc_list_clustering.append(iter_acc_list_clustering)\n",
        "  Target_acc_list_HIGHEST.append(iter_acc_list_HIGHEST)\n",
        "  print(\"Potential Highest Acc for Target subject: \"+str(Target_acc_list_HIGHEST))\n",
        "  #print(\"accuracy for this target (N=0)(MMPP): \")\n",
        "  #print(Target_acc_list)\n",
        "  #print(\"accuracy for this target (N=0)(By Clustering): \")\n",
        "  #print(Target_acc_list)\n",
        "  #print(\"\\n\\n********* accuracy for subject \"+str(target_sub)+\" as target: \"+str(np.mean(ACC_MAT_iter[target_sub-1,:]))+\" *********\\n\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**\n",
        "The following results were obtained for **subjects 1 to 5** as target subjects. These results should be saved in the corresponding results sheet.\n",
        "\n",
        "* **Note**: Considering only the first item of *iter_acc* is equivalent to **using only booster classifier** (**N=0**). This means that the trained classifiers that were used in the cluster selection stage were not used in the final classification. In order to maintain the comprehensiveness and generalizability of the code, it has been implemented in this way."
      ],
      "metadata": {
        "id": "LOic5uHzSsPM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi8Nhwa12kMa"
      },
      "outputs": [],
      "source": [
        "# Optimal Cluster Selection Metric: Choosing by Clustering\n",
        "TargetIter_AccMat = np.zeros((15, 5))\n",
        "\n",
        "# Iterate through each target subject\n",
        "for target in range(5):\n",
        "  target_acc = Target_acc_list_clustering[target]\n",
        "\n",
        "  # Iterate through each iteration for the current target\n",
        "  for iter in range(5):\n",
        "    iter_acc = target_acc[iter]\n",
        "    iteration_accuracy = iter_acc[0]\n",
        "    TargetIter_AccMat[target, iter] = iteration_accuracy\n",
        "\n",
        "# Calculate the mean accuracy for each target subject\n",
        "Target_Acc = np.mean(TargetIter_AccMat, axis=1)\n",
        "print(\"Mean accuracy for each target:\", Target_Acc)\n",
        "\n",
        "# Calculate the overall mean accuracy\n",
        "overall_acc = np.mean(Target_Acc)\n",
        "print(\"Overall mean accuracy:\", overall_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLd48gNohURx"
      },
      "outputs": [],
      "source": [
        "# Optimal Cluster Selection Metric: Maximum Major Prediction Proportion\n",
        "TargetIter_AccMat = np.zeros((15, 5))\n",
        "\n",
        "# Iterate through each target subject\n",
        "for target in range(5):\n",
        "  target_acc = Target_acc_list_selected_MMPP[target]\n",
        "\n",
        "  # Iterate through each iteration for the current target subject\n",
        "  for iter in range(5):\n",
        "    iter_acc = target_acc[iter]\n",
        "    iteration_accuracy = iter_acc[0]\n",
        "    TargetIter_AccMat[target, iter] = iteration_accuracy\n",
        "\n",
        "# Calculate the mean accuracy for each target subject\n",
        "Target_Acc = np.mean(TargetIter_AccMat, axis=1)\n",
        "print(\"Mean accuracy for each target:\", Target_Acc)\n",
        "\n",
        "# Calculate the overall mean accuracy\n",
        "overall_acc = np.mean(Target_Acc)\n",
        "print(\"Overall mean accuracy:\", overall_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw62ZXY-fVaR"
      },
      "outputs": [],
      "source": [
        "# Optimal Cluster Selection Metric: Maximum Correctly Predicted Classifiers Proportion\n",
        "TargetIter_AccMat = np.zeros((15, 5))\n",
        "\n",
        "# Iterate through each target subject\n",
        "for target in range(5):\n",
        "  target_acc = Target_acc_list_selected_MNCC[target]\n",
        "\n",
        "  # Iterate through each iteration for the current target subject\n",
        "  for iter in range(5):\n",
        "    iter_acc = target_acc[iter]\n",
        "    iteration_accuracy = iter_acc[0]\n",
        "    TargetIter_AccMat[target, iter] = iteration_accuracy\n",
        "\n",
        "# Calculate the mean accuracy for each target subject\n",
        "Target_Acc = np.mean(TargetIter_AccMat, axis=1)\n",
        "print(\"Mean accuracy for each target:\", Target_Acc)\n",
        "\n",
        "# Calculate the overall mean accuracy\n",
        "overall_acc = np.mean(Target_Acc)\n",
        "print(\"Overall mean accuracy:\", overall_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHHxfXjmfev4"
      },
      "outputs": [],
      "source": [
        "# Optimal Cluster Selection Metric: Maximum Mean Accuracy\n",
        "TargetIter_AccMat = np.zeros((15, 5))\n",
        "\n",
        "# Iterate through each target subject\n",
        "for target in range(5):\n",
        "  target_acc = Target_acc_list_selected_MMA[target]\n",
        "\n",
        "  # Iterate through each iteration for the current target subject\n",
        "  for iter in range(5):\n",
        "    iter_acc = target_acc[iter]\n",
        "    iteration_accuracy = iter_acc[0]\n",
        "    TargetIter_AccMat[target, iter] = iteration_accuracy\n",
        "\n",
        "# Calculate the mean accuracy for each target subject\n",
        "Target_Acc = np.mean(TargetIter_AccMat, axis=1)\n",
        "print(\"Mean accuracy for each target:\", Target_Acc)\n",
        "\n",
        "# Calculate the overall mean accuracy\n",
        "overall_acc = np.mean(Target_Acc)\n",
        "print(\"Overall mean accuracy:\", overall_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORoYygbOfh5Q"
      },
      "outputs": [],
      "source": [
        "# Optimal Cluster Selection Metric: Maximum Top Accuracy\n",
        "TargetIter_AccMat = np.zeros((15, 5))\n",
        "\n",
        "# Iterate through each target subject\n",
        "for target in range(5):\n",
        "  target_acc = Target_acc_list_selected_MTA[target]\n",
        "\n",
        "  # Iterate through each iteration for the current target subject\n",
        "  for iter in range(5):\n",
        "    iter_acc = target_acc[iter]\n",
        "    iteration_accuracy = iter_acc[0]\n",
        "    TargetIter_AccMat[target, iter] = iteration_accuracy\n",
        "\n",
        "# Calculate the mean accuracy for each target subject\n",
        "Target_Acc = np.mean(TargetIter_AccMat, axis=1)\n",
        "print(\"Mean accuracy for each target:\", Target_Acc)\n",
        "\n",
        "# Calculate the overall mean accuracy\n",
        "overall_acc = np.mean(Target_Acc)\n",
        "print(\"Overall mean accuracy:\", overall_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxcfaHSgfjpQ"
      },
      "outputs": [],
      "source": [
        "# Best available accuracy: Assuming that in each iteration, the best cluster is selected in some way\n",
        "# We use this accuracy, as well as the closeness of the accuracy of the abovementioned metrics, to evaluate the modification in the algorithm\n",
        "\n",
        "TargetIter_AccMat = np.zeros((15, 5))\n",
        "\n",
        "# Iterate through each target\n",
        "for target in range(5):\n",
        "  target_acc = Target_acc_list_HIGHEST[target]\n",
        "\n",
        "  # Iterate through each iteration for the current target\n",
        "  for iter in range(5):\n",
        "    iter_acc = target_acc[iter]\n",
        "    TargetIter_AccMat[target, iter] = iter_acc\n",
        "\n",
        "# Calculate the mean accuracy for each target\n",
        "Target_Acc = np.mean(TargetIter_AccMat, axis=1)\n",
        "print(\"Mean accuracy for each target:\", Target_Acc)\n",
        "\n",
        "# Calculate the overall mean accuracy\n",
        "overall_acc = np.mean(Target_Acc)\n",
        "print(\"Overall mean accuracy:\", overall_acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}